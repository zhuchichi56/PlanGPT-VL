{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6895655252366926\n",
      "0.09671569824218751\n"
     ]
    }
   ],
   "source": [
    "data = {'loss': 0.6897, \n",
    "        'grad_norm': 0.1371459499355251, \n",
    "        'learning_rate': 4.798819431378626e-07, \n",
    "        'rewards/chosen': -0.014980114996433258, \n",
    "        'rewards/rejected': -0.022156300023198128, \n",
    "        'rewards/accuracies': 0.6171875, \n",
    "        'rewards/margins': 0.007176183629781008, \n",
    "        'logps/chosen': -260.08294677734375, \n",
    "        'logps/rejected': -269.7545166015625, \n",
    "        'logits/chosen': -0.31492024660110474, \n",
    "        'logits/rejected': -0.3200848400592804,\n",
    "        'epoch': 0.22}\n",
    "\n",
    "\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "# compute the margin\n",
    "margin = data['rewards/chosen'] - data['rewards/rejected']\n",
    "\n",
    "loss = -math.log(sigmoid(margin))\n",
    "print(loss)\n",
    "\n",
    "beta =0.01\n",
    "\n",
    "rewards = beta * (data['logps/chosen'] - data['logps/rejected'])\n",
    "print(rewards)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"input_metrics\": {\n",
      "    \"loss\": 0.6897,\n",
      "    \"rewards_chosen\": -0.014980114996433258,\n",
      "    \"rewards_rejected\": -0.022156300023198128,\n",
      "    \"rewards_accuracies\": 0.6171875,\n",
      "    \"rewards_margins\": 0.007176183629781008,\n",
      "    \"logps_chosen\": -260.08294677734375,\n",
      "    \"logps_rejected\": -269.7545166015625\n",
      "  },\n",
      "  \"derived_metrics\": {\n",
      "    \"reference_logps_chosen\": -259.9331456273794,\n",
      "    \"reference_logps_rejected\": -269.5329536013305,\n",
      "    \"calculated_rewards_chosen\": -0.014980114996433258,\n",
      "    \"calculated_rewards_rejected\": -0.022156300023198128,\n",
      "    \"calculated_margins\": 0.00717618502676487,\n",
      "    \"margin_error\": 1.3969838619232178e-09,\n",
      "    \"calculated_loss\": 0.6895655259326784,\n",
      "    \"loss_error\": 0.0001344740673215883\n",
      "  },\n",
      "  \"relationships\": {\n",
      "    \"rewards_chosen_formula\": \"beta * (logps_chosen - reference_logps_chosen)\",\n",
      "    \"rewards_rejected_formula\": \"beta * (logps_rejected - reference_logps_rejected)\",\n",
      "    \"margins_formula\": \"rewards_chosen - rewards_rejected\",\n",
      "    \"loss_formula\": \"-log(sigmoid(margins))\"\n",
      "  },\n",
      "  \"interpretation\": {\n",
      "    \"accuracy_above_random\": true,\n",
      "    \"margin_positive\": true,\n",
      "    \"loss_decreasing\": true,\n",
      "    \"model_learning\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "===== DPO指标关系总结 =====\n",
      "1. 奖励计算: rewards = beta * (logps - reference_logps)\n",
      "   - chosen奖励: -0.014980\n",
      "   - rejected奖励: -0.022156\n",
      "\n",
      "2. 边际奖励: margins = rewards_chosen - rewards_rejected\n",
      "   - 计算值: 0.007176\n",
      "   - 实际值: 0.007176\n",
      "\n",
      "3. 准确率: accuracy = proportion of (margins > 0)\n",
      "   - 准确率: 0.6172 (约61.7%样本判断正确)\n",
      "\n",
      "4. 损失函数: loss = -log(sigmoid(margins))\n",
      "   - 计算值: 0.6896\n",
      "   - 实际值: 0.6897\n",
      "\n",
      "5. 训练进度: epoch = 0.22\n",
      "   - 模型学习状态: 有效学习中\n"
     ]
    }
   ],
   "source": [
    "def analyze_dpo_metrics(data):\n",
    "    \"\"\"\n",
    "    分析DPO训练指标之间的关系\n",
    "    \n",
    "    参数:\n",
    "    - data: 包含DPO训练指标的字典\n",
    "    \n",
    "    返回:\n",
    "    - analysis: 包含指标间关系分析的字典\n",
    "    \"\"\"\n",
    "    # 提取基础指标\n",
    "    loss = data['loss']\n",
    "    rewards_chosen = data['rewards/chosen']\n",
    "    rewards_rejected = data['rewards/rejected']\n",
    "    rewards_accuracies = data['rewards/accuracies']\n",
    "    rewards_margins = data['rewards/margins']\n",
    "    logps_chosen = data['logps/chosen']\n",
    "    logps_rejected = data['logps/rejected']\n",
    "    \n",
    "    # 假设beta值为0.1（常用值）\n",
    "    beta = 0.1\n",
    "    \n",
    "    # 1. 反推参考模型的logps\n",
    "    reference_logps_chosen = logps_chosen - rewards_chosen / beta\n",
    "    reference_logps_rejected = logps_rejected - rewards_rejected / beta\n",
    "    \n",
    "    # 2. 验证rewards计算\n",
    "    calculated_rewards_chosen = beta * (logps_chosen - reference_logps_chosen)\n",
    "    calculated_rewards_rejected = beta * (logps_rejected - reference_logps_rejected)\n",
    "    \n",
    "    # 3. 验证margins计算\n",
    "    calculated_margins = rewards_chosen - rewards_rejected\n",
    "    margin_error = abs(calculated_margins - rewards_margins)\n",
    "    \n",
    "    # 4. 验证loss计算\n",
    "    import math\n",
    "    import numpy as np\n",
    "    \n",
    "    # 使用sigmoid函数计算loss\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    calculated_loss = -math.log(sigmoid(rewards_margins))\n",
    "    loss_error = abs(calculated_loss - loss)\n",
    "    \n",
    "    # 构建分析结果\n",
    "    analysis = {\n",
    "        \"input_metrics\": {\n",
    "            \"loss\": loss,\n",
    "            \"rewards_chosen\": rewards_chosen,\n",
    "            \"rewards_rejected\": rewards_rejected,\n",
    "            \"rewards_accuracies\": rewards_accuracies,\n",
    "            \"rewards_margins\": rewards_margins,\n",
    "            \"logps_chosen\": logps_chosen,\n",
    "            \"logps_rejected\": logps_rejected\n",
    "        },\n",
    "        \"derived_metrics\": {\n",
    "            \"reference_logps_chosen\": reference_logps_chosen,\n",
    "            \"reference_logps_rejected\": reference_logps_rejected,\n",
    "            \"calculated_rewards_chosen\": calculated_rewards_chosen,\n",
    "            \"calculated_rewards_rejected\": calculated_rewards_rejected,\n",
    "            \"calculated_margins\": calculated_margins,\n",
    "            \"margin_error\": margin_error,\n",
    "            \"calculated_loss\": calculated_loss,\n",
    "            \"loss_error\": loss_error\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"rewards_chosen_formula\": \"beta * (logps_chosen - reference_logps_chosen)\",\n",
    "            \"rewards_rejected_formula\": \"beta * (logps_rejected - reference_logps_rejected)\",\n",
    "            \"margins_formula\": \"rewards_chosen - rewards_rejected\",\n",
    "            \"loss_formula\": \"-log(sigmoid(margins))\"\n",
    "        },\n",
    "        \"interpretation\": {\n",
    "            \"accuracy_above_random\": rewards_accuracies > 0.5,\n",
    "            \"margin_positive\": rewards_margins > 0,\n",
    "            \"loss_decreasing\": loss < 0.693,  # 0.693 ≈ ln(2) 是随机猜测的损失\n",
    "            \"model_learning\": rewards_accuracies > 0.5 and rewards_margins > 0 and loss < 0.693\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# 使用提供的数据\n",
    "data = {\n",
    "    'loss': 0.6897, \n",
    "    'grad_norm': 0.1371459499355251, \n",
    "    'learning_rate': 4.798819431378626e-07, \n",
    "    'rewards/chosen': -0.014980114996433258, \n",
    "    'rewards/rejected': -0.022156300023198128, \n",
    "    'rewards/accuracies': 0.6171875, \n",
    "    'rewards/margins': 0.007176183629781008, \n",
    "    'logps/chosen': -260.08294677734375, \n",
    "    'logps/rejected': -269.7545166015625, \n",
    "    'logits/chosen': -0.31492024660110474, \n",
    "    'logits/rejected': -0.3200848400592804, \n",
    "    'epoch': 0.22\n",
    "}\n",
    "\n",
    "# 分析指标关系\n",
    "analysis = analyze_dpo_metrics(data)\n",
    "\n",
    "# 打印详细结果\n",
    "import json\n",
    "print(json.dumps(analysis, indent=2))\n",
    "\n",
    "# 简明总结\n",
    "print(\"\\n===== DPO指标关系总结 =====\")\n",
    "print(f\"1. 奖励计算: rewards = beta * (logps - reference_logps)\")\n",
    "print(f\"   - chosen奖励: {data['rewards/chosen']:.6f}\")\n",
    "print(f\"   - rejected奖励: {data['rewards/rejected']:.6f}\")\n",
    "\n",
    "print(f\"\\n2. 边际奖励: margins = rewards_chosen - rewards_rejected\")\n",
    "print(f\"   - 计算值: {analysis['derived_metrics']['calculated_margins']:.6f}\")\n",
    "print(f\"   - 实际值: {data['rewards/margins']:.6f}\")\n",
    "\n",
    "print(f\"\\n3. 准确率: accuracy = proportion of (margins > 0)\")\n",
    "print(f\"   - 准确率: {data['rewards/accuracies']:.4f} (约{data['rewards/accuracies']*100:.1f}%样本判断正确)\")\n",
    "\n",
    "print(f\"\\n4. 损失函数: loss = -log(sigmoid(margins))\")\n",
    "print(f\"   - 计算值: {analysis['derived_metrics']['calculated_loss']:.4f}\")\n",
    "print(f\"   - 实际值: {data['loss']:.4f}\")\n",
    "\n",
    "print(f\"\\n5. 训练进度: epoch = {data['epoch']:.2f}\")\n",
    "print(f\"   - 模型学习状态: {'有效学习中' if analysis['interpretation']['model_learning'] else '尚未有效学习'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
